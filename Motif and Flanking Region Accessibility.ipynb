{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import itertools\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick \n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILE UPLOAD AND DICTIONARY GENERATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text file of target and nontarget names\n",
    "targets = pd.read_csv('C:\\\\Users\\\\Alireza\\\\Documents\\\\MGY480\\\\Stoiber et al\\\\Individual RBPs\\\\Rbp1\\\\new_method\\\\posgenes.txt', header = None)\n",
    "nontargets = pd.read_csv('C:\\\\Users\\\\Alireza\\\\Documents\\\\MGY480\\\\Stoiber et al\\\\Individual RBPs\\\\Rbp1\\\\new_method\\\\neggenes.txt', header = None)\n",
    "RNAplfold_dir = 'C:\\\\Users\\\\Alireza\\\\Documents\\\\MGY480\\\\Stoiber et al\\\\Individual RBPs\\\\Rbp1\\\\new_method\\\\RNAplfold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary where keys are the fbtr and value is the list of sequence records\n",
    "# if using fbtr:\n",
    "\n",
    "fbtr2seqrecord = {}\n",
    "for seqr in SeqIO.parse('C:\\\\Users\\\\Alireza\\\\Documents\\\\MGY480\\\\Hua_RIP\\\\brat_smg_targets20201210\\\\dmel-all-rna-r6.23.fasta','fasta'):\n",
    "    fbtr = seqr.id\n",
    "    fbtr2seqrecord[fbtr] = str(seqr.seq).replace('T','U') # str(seqr.seq) removes unwanted stuff (i.e. compare this line to the same thing but not using str); also, str allows you to use .replace\n",
    "\n",
    "#filter dictionary such that only fbtr in target list are keys\n",
    "target2seq = {key: fbtr2seqrecord[key] for key in targets[0]}\n",
    "nontarget2seq = {key: fbtr2seqrecord[key] for key in nontargets[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary where keys are the fbtr and value is the list of sequence records\n",
    "# if using fbgn (e.g. from stoiber set), then need longest transcript per gene:\n",
    "\n",
    "gene2seqrecord = {}\n",
    "for seqr in SeqIO.parse('C:\\\\Users\\\\Alireza\\\\Documents\\\\NSERC 2020\\\\Project 1\\\\Data Files\\\\dmel-all-transcript-r6.33.fasta','fasta'):\n",
    "    gene = seqr.description.split(' ')[8][7:18]\n",
    "    if gene in gene2seqrecord.keys():\n",
    "        gene2seqrecord[gene].append(seqr)\n",
    "    else:\n",
    "        gene2seqrecord[gene] = [seqr]\n",
    "\n",
    "# loop for all genes, sort the SeqRecord list by sequence length, longest first\n",
    "# create new dic, fbgn2seq record where key is fbgn and value is seq of longest transcript\n",
    "fbgn2seqrecord = {}\n",
    "for gene, seqrlist in gene2seqrecord.items():\n",
    "    seqrlist = sorted(seqrlist, key=lambda x:len(str(x.seq)), reverse=True)\n",
    "    fbgn2seqrecord[gene] = str(seqrlist[0].seq).replace('T','U')\n",
    "\n",
    "#filter dictionary such that only fbtr in target list are keys\n",
    "target2seq = {key: fbgn2seqrecord[key] for key in targets[0]}\n",
    "nontarget2seq = {key: fbgn2seqrecord[key] for key in nontargets[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus_to_motifs(consensus): # modification of #ATS fxn\n",
    "    '''input = #ATS motif, output = all possible combinations of degenerate motifs'''\n",
    "\n",
    "    seq_dic = {}\n",
    "    seq_dic['A'] = 'A'\n",
    "    seq_dic['C'] = 'C'\n",
    "    seq_dic['U'] = 'U'\n",
    "    seq_dic['G'] = 'G'\n",
    "    seq_dic['H'] = 'ACU'\n",
    "    seq_dic['W'] = 'AU'\n",
    "    seq_dic['N'] = 'ACGU'\n",
    "    seq_dic['M'] = 'AC'\n",
    "    seq_dic['Y'] = 'UC'\n",
    "    seq_dic['K'] = 'GU'\n",
    "    seq_dic['B'] = 'CGU'\n",
    "    seq_dic['D'] = 'AGU'\n",
    "    seq_dic['R'] = 'GA'\n",
    "    seq_dic['V'] = 'ACG'\n",
    "    seq_dic['S'] = 'GC'\n",
    "    \n",
    "    motif = []\n",
    "    for nt in consensus:\n",
    "        motif.append(seq_dic[nt])\n",
    "        \n",
    "    motif_list = list(itertools.product(*motif))\n",
    "    degen_motifs = []\n",
    "    for comb in motif_list:\n",
    "        degen_motifs.append((\"\".join(comb)))\n",
    "        \n",
    "    return degen_motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motif_list = consensus_to_motifs()\n",
    "def transcript_removal(transcript2seq, motif_list):\n",
    "    '''Remove transcripts that don't have any motifs from motif_list '''\n",
    "    \n",
    "    filtered = transcript2seq.copy()\n",
    "    for fbtr, seq in list(filtered.items()): #.items puts key and value in brackets i.e. dict_items([(key1,val1), (key2,val2)]) --> allows unpacking of key, val pairs; need to use list because we are changing dic size\n",
    "        n = 0 \n",
    "        for motif in motif_list:\n",
    "            if motif not in seq:\n",
    "                n +=1 \n",
    "        if n == len(motif_list): # this condition only occurs if none of the motifs in motif_list are present in a given transcript\n",
    "            filtered.pop(fbtr)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dic where keys are targets and values are location of motifs \n",
    "def motif_occurences(transcript2seq, motif_list):\n",
    "    '''return a new dic with key = fbtr and values = [seq,[occurences]]\n",
    "    Occurence is stored as number of last nucleotide in motif'''\n",
    "    \n",
    "    copy_dic = transcript2seq.copy()\n",
    "    for fbtr, seq in copy_dic.items():\n",
    "        occurences = []\n",
    "        for motif in motif_list:\n",
    "            matches = re.finditer(motif, seq)\n",
    "            matches_positions = [match.start() for match in matches]\n",
    "            matches_positions = [m + len(motif)-1 for m in matches_positions]\n",
    "            occurences += matches_positions\n",
    "        copy_dic[fbtr] = [copy_dic[fbtr], occurences]\n",
    "        \n",
    "    return copy_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plfold_results = open(RNAplfold_dir + pos_gene + '_lunp', 'r')\n",
    "def max_access(RNAplfold_direct, transcript2seq_occurences, mer_length):\n",
    "    '''return a new dic, transcript2seq_occurence_max, with key = fbtr and values = ['seq', [motif w/ max accessibility], [max accessibility value]]\n",
    "    RNAplfold_direct is the directory of RNAplfold results, transcript2seq_occurences is dic of key = fbtr and\n",
    "    values = ['seq', [occurences]]'''\n",
    "    \n",
    "    copy_dic = transcript2seq_occurences.copy()\n",
    "    for fbtr, values in copy_dic.items():\n",
    "        plfold_results = open(RNAplfold_direct + '\\\\' + fbtr + '_lunp', 'r')\n",
    "        plfold_df = pd.read_csv(plfold_results, sep = '\\t', skiprows = 1)\n",
    "        plfold_df = plfold_df.rename(columns={' #i$': 'nt', 'l=1': '1'})\n",
    "        acc_list = []\n",
    "        for occurence in values[1]: # values[1] is all motif occurences in transcript\n",
    "            acc_list.append(plfold_df[str(mer_length)].iloc[occurence - 1]) # df[\"column\"].iloc[row]; occurence - 1 = df index\n",
    "        copy_dic[fbtr] = [copy_dic[fbtr][0],copy_dic[fbtr][1], acc_list]\n",
    "    \n",
    "    final = {}\n",
    "    for fbtr, values in copy_dic.items():\n",
    "        val = np.argmax(copy_dic[fbtr][2]) #argmin and argmax give index of min and max value, respectively\n",
    "        final[fbtr] = [copy_dic[fbtr][0], copy_dic[fbtr][1][val], copy_dic[fbtr][2][val]]\n",
    "        \n",
    "    # calculation of average accessibility of max values\n",
    "    total_acc = 0 \n",
    "    num_transcripts = 0\n",
    "    for fbtr, values in final.items():\n",
    "        if not np.isnan(final[fbtr][2]):\n",
    "            total_acc += final[fbtr][2]\n",
    "            num_transcripts += 1\n",
    "    avg_acc = (total_acc/num_transcripts)\n",
    "    print ('The average accessibility of the most accessible motifs is: ' + str(avg_acc))\n",
    "    \n",
    "    # calculation of median accessibility of max values\n",
    "    final_df = pd.DataFrame(final).T\n",
    "    final_df_nona = final_df.dropna(axis = 0, subset=[2])\n",
    "    print ('The median accessibility of the most accessible motifs is: ' + str(final_df_nona[2].median()))\n",
    "        \n",
    "    return final\n",
    "\n",
    "def motif_access_wilcoxon(target2seq_occurence_max, nontarget2seq_occurence_max):\n",
    "    '''Wilcoxon ranked sum test for comparison of motifs with maximum accessibility b/w targets and nontargets'''\n",
    "    \n",
    "    num_significant = 0\n",
    "    num_non_significant = 0\n",
    "    p_value_list = []\n",
    "    target_df = pd.DataFrame(target2seq_occurence_max).T\n",
    "    nontarget_df = pd.DataFrame(nontarget2seq_occurence_max).T\n",
    "    \n",
    "    # for _ in range(num_replicates):\n",
    "        \n",
    "    target_nona = target_df.dropna(axis=0, subset=[2])\n",
    "    nontarget_nona = nontarget_df.dropna(axis=0, subset=[2])\n",
    "        # nontarget_nona_sample = nontarget_nona.sample(n = len(target_nona))\n",
    "        \n",
    "    stat, p_value = sp.stats.mannwhitneyu(target_nona[2], nontarget_nona[2])\n",
    "    p_value_list.append(p_value)\n",
    "        \n",
    "    if p_value < 0.05:\n",
    "        num_significant +=1\n",
    "    else:\n",
    "        num_non_significant +=1 \n",
    "            \n",
    "    print ('Number of significant runs = ' + str(num_significant))\n",
    "    print ('Number of non-significant runs =  ' + str(num_non_significant))\n",
    "    print (sum(p_value_list) / float(len(p_value_list)))\n",
    "    \n",
    "    return target_df, nontarget_df # df: rows = fbtr, columns = [seq, max_motif, max_motif_value]\n",
    "\n",
    "    \n",
    "def motif_access_distribution(df, bin_increment, bin_max, percentage_tick, max_tick, RBP_name):\n",
    "    #df returned in motif_access_wilcoxon\n",
    "    #bin_increment is the increments on the y-axis, with bin_max being the max bin --> Judgement call based on data\n",
    "    #percentage_ticks is the increment on x-axis inserted as a float (e.g. 2% increments = 0.02)\n",
    "    # max_tick is the max y value --> judgement call; run with 1 first and then decide\n",
    "    \n",
    "    #create bin for histogram\n",
    "    n = 0\n",
    "    bin_list = [n]\n",
    "    while n < bin_max: \n",
    "        n += bin_increment\n",
    "        bin_list.append(n)\n",
    "    \n",
    "    no_na = df.dropna()\n",
    "    data = no_na[2]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    y_vals, x_vals, e_ = ax.hist(data, bin_list , edgecolor='black')\n",
    "    # y_max = round((max(y_vals) / len(data)) + 0.05, 2)\n",
    "    y_max = max_tick + 0.05\n",
    "    ax.set_yticks(ticks=np.arange(0.0, y_max * len(data), percentage_tick * len(data)))\n",
    "    ax.set_ylim(ax.get_yticks()[0], ax.get_yticks()[-1])\n",
    "    ax.set_xlabel('accessibility')\n",
    "    ax.set_title(RBP_name)\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=len(data)))\n",
    "    \n",
    "def seq_excerpt(transcript2seq_occurence_max, out_dir):\n",
    "    '''create new file with sequence of motif and flanking region for each transcript in dic'''\n",
    "\n",
    "    fname = out_dir\n",
    "    fw = open(fname,'w')\n",
    "\n",
    "    for fbtr, values in transcript2seq_occurence_max.items():\n",
    "        val1 = transcript2seq_occurence_max[fbtr][1] - 1 - 25\n",
    "        val2 = transcript2seq_occurence_max[fbtr][1] - 1 + 15\n",
    "\n",
    "        if val1 > 0: \n",
    "            fw.write('>'+fbtr+'\\n')\n",
    "            fw.write(transcript2seq_occurence_max[fbtr][0][val1:val2 + 1] + '\\n')\n",
    "        else:\n",
    "            fw.write('>'+fbtr+'\\n')\n",
    "            fw.write(transcript2seq_occurence_max[fbtr][0][0: val2 + 1] + '\\n')\n",
    "\n",
    "\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flanking_access(transcript2seq_occurence_max, RNAplfold_direct, flank_length, mer_length):\n",
    "    '''Return accessibility of region flanking motif w/ max accessibility\n",
    "    transcrip2seq_occurence_max is key = fbtr and values = ['seq', [occurence of motif w/ max access], [max access value]]'''\n",
    "    \n",
    "    five_prime_acc_list = []\n",
    "    three_prime_acc_list = []\n",
    "    for fbtr, values in transcript2seq_occurence_max.items():\n",
    "        plfold_results = open(RNAplfold_direct + '\\\\' + fbtr + '_lunp', 'r')\n",
    "        plfold_df = pd.read_csv(plfold_results, sep = '\\t', skiprows = 1)\n",
    "        plfold_df = plfold_df.rename(columns={' #i$': 'nt', 'l=1': '1'})\n",
    "        \n",
    "        five_row =  transcript2seq_occurence_max[fbtr][1] - 1 - mer_length\n",
    "        if five_row > 0:\n",
    "            if not np.isnan(plfold_df[str(flank_length)].iloc[five_row]):\n",
    "                five_prime_acc_list.append(plfold_df[str(flank_length)].iloc[five_row])\n",
    "        \n",
    "        three_row = transcript2seq_occurence_max[fbtr][1] - 1 + flank_length\n",
    "        if three_row < len(transcript2seq_occurence_max[fbtr][0]):\n",
    "            if not np.isnan(plfold_df[str(flank_length)].iloc[three_row]):\n",
    "                three_prime_acc_list.append(plfold_df[str(flank_length)].iloc[three_row])\n",
    "    \n",
    "    #avg_five = np.mean(five_prime_acc_list)\n",
    "    #avg_three = np.mean(three_prime_acc_list)\n",
    "    combined = five_prime_acc_list + three_prime_acc_list\n",
    "    avg_flank_acc = np.mean(combined)\n",
    "    median_flank_acc = np.median(combined)\n",
    "    \n",
    "    # return avg_five, avg_three\n",
    "    print('median access = ' + str(median_flank_acc))\n",
    "    return avg_flank_acc\n",
    "\n",
    "def flank_access_wilcoxon(target2seq_occurence_max, nontarget2seq_occurence_max, RNAplfold_direct, flank_length, mer_length):\n",
    "    \n",
    "    target2maxflank = {} # new dic to be populated; key = fbgtr and value = [max_flank_access]\n",
    "    nontarget2maxflank = {}\n",
    "    \n",
    "    for fbtr, values in target2seq_occurence_max.items():\n",
    "        plfold_results = open(RNAplfold_direct + '\\\\' + fbtr + '_lunp', 'r')\n",
    "        plfold_df = pd.read_csv(plfold_results, sep = '\\t', skiprows = 1)\n",
    "        plfold_df = plfold_df.rename(columns={' #i$': 'nt', 'l=1': '1'})\n",
    "        \n",
    "        five_row =  target2seq_occurence_max[fbtr][1] - 1 - mer_length\n",
    "        if five_row > 0:\n",
    "            if not np.isnan(plfold_df[str(flank_length)].iloc[five_row]):\n",
    "                five_prime = plfold_df[str(flank_length)].iloc[five_row]\n",
    "                \n",
    "        three_row = target2seq_occurence_max[fbtr][1] - 1 + flank_length\n",
    "        if three_row < len(target2seq_occurence_max[fbtr][0]):\n",
    "            if not np.isnan(plfold_df[str(flank_length)].iloc[three_row]):\n",
    "                three_prime = plfold_df[str(flank_length)].iloc[three_row]\n",
    "        \n",
    "        if not np.isnan(five_prime) and not np.isnan(three_prime):\n",
    "            flank_avg = (five_prime+three_prime)/2\n",
    "            target2maxflank[fbtr] = [flank_avg]\n",
    "    \n",
    "    for fbtr, values in nontarget2seq_occurence_max.items():\n",
    "        plfold_results = open(RNAplfold_direct + '\\\\' + fbtr + '_lunp', 'r')\n",
    "        plfold_df = pd.read_csv(plfold_results, sep = '\\t', skiprows = 1)\n",
    "        plfold_df = plfold_df.rename(columns={' #i$': 'nt', 'l=1': '1'})\n",
    "        \n",
    "        five_row =  nontarget2seq_occurence_max[fbtr][1] - 1 - mer_length\n",
    "        if five_row > 0:\n",
    "            if not np.isnan(plfold_df[str(flank_length)].iloc[five_row]):\n",
    "                five_prime = plfold_df[str(flank_length)].iloc[five_row]\n",
    "                \n",
    "        three_row = nontarget2seq_occurence_max[fbtr][1] - 1 + flank_length\n",
    "        if three_row < len(nontarget2seq_occurence_max[fbtr][0]):\n",
    "            if not np.isnan(plfold_df[str(flank_length)].iloc[three_row]):\n",
    "                three_prime = plfold_df[str(flank_length)].iloc[three_row]\n",
    "        \n",
    "        if not np.isnan(five_prime) and not np.isnan(three_prime):\n",
    "            flank_avg = (five_prime+three_prime)/2\n",
    "            nontarget2maxflank[fbtr] = [flank_avg]\n",
    "    \n",
    "    \n",
    "    num_significant = 0\n",
    "    num_non_significant = 0\n",
    "    p_value_list = []\n",
    "    target_df = pd.DataFrame(target2maxflank).T\n",
    "    nontarget_df = pd.DataFrame(nontarget2maxflank).T\n",
    "    \n",
    "    # for _ in range(num_replicates):\n",
    "    target_nona = target_df.dropna(axis=0, subset=[0])\n",
    "    nontarget_nona = nontarget_df.dropna(axis=0, subset=[0])\n",
    "        # nontarget_nona_sample = nontarget_nona.sample(n = len(target_nona))\n",
    "        \n",
    "    stat, p_value = sp.stats.mannwhitneyu(target_nona[0], nontarget_nona[0])\n",
    "    p_value_list.append(p_value)\n",
    "        \n",
    "    if p_value < 0.05:\n",
    "        num_significant +=1\n",
    "    else:\n",
    "        num_non_significant +=1 \n",
    "            \n",
    "    print ('Number of significant runs = ' + str(num_significant))\n",
    "    print ('Number of non-significant runs =  ' + str(num_non_significant))\n",
    "    print (sum(p_value_list) / float(len(p_value_list)))\n",
    "    \n",
    "    return target_df, nontarget_df # for use in flank_access_histogram (df w/ structure: rows = fbtr and column = max_flank_value)\n",
    "\n",
    "def flank_access_distribution(df, bin_increment, bin_max, percentage_tick, max_tick, RBP_name):\n",
    "     #df returned in flank_max_wilcoxon\n",
    "    #bin_increment is the increments on the y-axis, with bin_max being the max bin --> Judgement call based on data\n",
    "    #percentage_ticks is the increment on x-axis inserted as a float (e.g. 2% increments = 0.02)\n",
    "    # max_tick is the max y value --> judgement call; run with 1 first and then decide.\n",
    "    \n",
    "    #create bin for histogram\n",
    "    n = 0\n",
    "    bin_list = [n]\n",
    "    while n < bin_max: \n",
    "        n += bin_increment\n",
    "        bin_list.append(n)\n",
    "    \n",
    "    no_na = df.dropna()\n",
    "    data = no_na[0]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    y_vals, x_vals, e_ = ax.hist(data, bin_list , edgecolor='black')\n",
    "    # y_max = round((max(y_vals) / len(data)) + 0.05, 2)\n",
    "    y_max = max_tick + 0.05\n",
    "    ax.set_yticks(ticks=np.arange(0.0, y_max * len(data), percentage_tick * len(data)))\n",
    "    ax.set_ylim(ax.get_yticks()[0], ax.get_yticks()[-1])\n",
    "    ax.set_xlabel('accessibility')\n",
    "    ax.set_title(RBP_name)\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=len(data)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_occurence_comparison(nested_dic):\n",
    "    ''' nested_dic is a dic where key = motif and value = transcrip2seq_occurence_max \n",
    "    for which key = fbtr and values = ['seq', [occurence of motif w/ max access], [max access value]]\n",
    "    Return dataframe that shows occurence of each variant of a motif in each transcript'''\n",
    "    \n",
    "    final_df = pd.DataFrame(nested_dic).applymap(lambda x: x[1])\n",
    "        \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBP_motif = 'CAWCWNCH'\n",
    "motif_list = consensus_to_motifs(RBP_motif) \n",
    "\n",
    "pos_motifs = transcript_removal(target2seq, motif_list)\n",
    "target_occurences = motif_occurences(pos_motifs, motif_list)\n",
    "target_occurences_access = max_access(RNAplfold_dir, target_occurences, len(RBP_motif)) # Change mer_length\n",
    "\n",
    "neg_motifs = transcript_removal(nontarget2seq, motif_list)\n",
    "nontarget_occurences = motif_occurences(neg_motifs, motif_list)\n",
    "nontarget_occurences_access = max_access(RNAplfold_dir, nontarget_occurences, len(RBP_motif)) #Change mer_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for motif in motif_list:\n",
    "    print(motif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flank accessibility output (once per flank length; change in third and fourth)\n",
    "# flanking_access(transcript2seq_occurence_max, RNAplfold_direct, flank_length, mer_length)\n",
    "for i in range(4,9):\n",
    "    target_final_val = flanking_access(target_occurences_access, RNAplfold_dir, i, len(RBP_motif)) # Change flank_length and mer_length\n",
    "    nontarget_final_val = flanking_access(nontarget_occurences_access, RNAplfold_dir, i, len(RBP_motif)) # Change flank_length and mer_length\n",
    "    print (target_final_val, nontarget_final_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motif accessibilty \n",
    "targets_df, nontargets_df = motif_access_wilcoxon(target_occurences_access, nontarget_occurences_access) \n",
    "\n",
    "motif_access_distribution(targets_df, 0.1, 1, 0.05, 0.4, 'FMR1 Targets') #Change RBP name\n",
    "motif_access_distribution(nontargets_df, 0.1, 1, 0.05, 0.4, 'FMR1 Nontargets') #Change RBP name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Flank access (once per flank length; change in first line)\n",
    "# flank_access_wilcoxon(target2seq_occurence_max, nontarget2seq_occurence_max, RNAplfold_direct, flank_length, mer_length)\n",
    "for i in range(4,9):\n",
    "    targets_flank_df, nontargets_flank_df = flank_access_wilcoxon(target_occurences_access, nontarget_occurences_access, RNAplfold_dir, i, len(RBP_motif))\n",
    "    flank_access_distribution(targets_flank_df, 0.1, 1, 0.05, 0.5, 'FMR1 Targets')\n",
    "    flank_access_distribution(nontargets_flank_df, 0.1, 1, 0.05, 0.5, 'FMR1 Nontargets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir_name = 'C:\\\\Users\\\\Alireza\\\\Documents\\\\MGY480\\\\seq_excerpt\\\\QKR58E1_nontargets.txt'\n",
    "seq_excerpt(nontarget_occurences_access, out_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMG motif occurence analysis\n",
    "\n",
    "smg_motifs = ('CNGG', 'CNGGN', 'CNGGNN', 'CNGGNNN', 'CNGGNNNN', 'CNGGNNNNN')\n",
    "target_nest_dic = {}\n",
    "nontarget_nest_dic = {}\n",
    "\n",
    "for i in range(len(smg_motifs)):\n",
    "    motif_list = consensus_to_motifs(smg_motifs[i]) # change motif\n",
    "\n",
    "    pos_motifs = transcript_removal(target2seq, motif_list)\n",
    "    target_occurences = motif_occurences(pos_motifs, motif_list)\n",
    "    target_occurences_access = max_access(RNAplfold_dir, target_occurences, len(smg_motifs[i]))\n",
    "    target_nest_dic[smg_motifs[i]] = target_occurences_access\n",
    "    \n",
    "\n",
    "    neg_motifs = transcript_removal(nontarget2seq, motif_list)\n",
    "    nontarget_occurences = motif_occurences(neg_motifs, motif_list)\n",
    "    nontarget_occurences_access = max_access(RNAplfold_dir, nontarget_occurences, len(smg_motifs[i]))\n",
    "    nontarget_nest_dic[smg_motifs[i]] = nontarget_occurences_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_smg = motif_occurence_comparison(target_nest_dic)\n",
    "nontarget_smg = motif_occurence_comparison(nontarget_nest_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_smg.to_excel('C:\\\\Users\\\\Alireza\\\\Documents\\\\MGY480\\\\smg_motif_variant_occurences\\\\targets.xlsx')\n",
    "nontarget_smg.to_excel('C:\\\\Users\\\\Alireza\\\\Documents\\\\MGY480\\\\smg_motif_variant_occurences\\\\nontargets.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Logo generation\n",
    "RBP_motif = 'UBCYB'\n",
    "motif_list = consensus_to_motifs(RBP_motif) \n",
    "for motif in motif_list:\n",
    "    print(motif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smg_targets_df = pd.read_excel('C:\\\\Users\\\\Alireza\\\\Documents\\\\MGY480\\\\smg_motif_variant_occurences\\\\targets.xlsx')\n",
    "smg_nontargets_df = pd.read_excel('C:\\\\Users\\\\Alireza\\\\Documents\\\\MGY480\\\\smg_motif_variant_occurences\\\\nontargets.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smg_targets_df_new = pd.DataFrame()\n",
    "smg_targets_df_new[\"CNGG\"] = smg_targets_df[\"CNGG\"] \n",
    "smg_targets_df_new[\"CNGGN\"] = smg_targets_df[\"CNGGN\"] - 1\n",
    "smg_targets_df_new[\"CNGGNN\"] = smg_targets_df[\"CNGGNN\"] - 2\n",
    "smg_targets_df_new[\"CNGGNNN\"] = smg_targets_df[\"CNGGNNN\"] - 3\n",
    "smg_targets_df_new[\"CNGGNNNN\"] = smg_targets_df[\"CNGGNNNN\"] - 4\n",
    "smg_targets_df_new[\"CNGGNNNNN\"] = smg_targets_df[\"CNGGNNNNN\"] - 5\n",
    "smg_targets_df_new['Score'] = smg_targets_df_new.apply(lambda row: row.value_counts().iloc[0], axis = 1)\n",
    "\n",
    "smg_nontargets_df_new = pd.DataFrame()\n",
    "smg_nontargets_df_new[\"CNGG\"] = smg_nontargets_df[\"CNGG\"] \n",
    "smg_nontargets_df_new[\"CNGGN\"] = smg_nontargets_df[\"CNGGN\"] - 1\n",
    "smg_nontargets_df_new[\"CNGGNN\"] = smg_nontargets_df[\"CNGGNN\"] - 2\n",
    "smg_nontargets_df_new[\"CNGGNNN\"] = smg_nontargets_df[\"CNGGNNN\"] - 3\n",
    "smg_nontargets_df_new[\"CNGGNNNN\"] = smg_nontargets_df[\"CNGGNNNN\"] - 4\n",
    "smg_nontargets_df_new[\"CNGGNNNNN\"] = smg_nontargets_df[\"CNGGNNNNN\"] - 5\n",
    "smg_nontargets_df_new['Score'] = smg_nontargets_df_new.apply(lambda row: row.value_counts().iloc[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_list = [1,2,3,4,5,6,7]\n",
    "data = smg_targets_df_new['Score']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y_vals, x_vals, e_ = ax.hist(data, bin_list , edgecolor='black')\n",
    "y_max = 0.35 + 0.05\n",
    "ax.set_yticks(ticks=np.arange(0.0, y_max * len(data), 0.05 * len(data)))\n",
    "ax.set_ylim(ax.get_yticks()[0], ax.get_yticks()[-1])\n",
    "ax.set_xlabel('Score')\n",
    "ax.set_title('SMG Targets')\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_list = [1,2,3,4,5,6,7]\n",
    "data = smg_nontargets_df_new['Score']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y_vals, x_vals, e_ = ax.hist(data, bin_list , edgecolor='black')\n",
    "y_max = 0.35 + 0.05\n",
    "ax.set_yticks(ticks=np.arange(0.0, y_max * len(data), 0.05 * len(data)))\n",
    "ax.set_ylim(ax.get_yticks()[0], ax.get_yticks()[-1])\n",
    "ax.set_xlabel('Score')\n",
    "ax.set_title('SMG Nontargets')\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=len(data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
